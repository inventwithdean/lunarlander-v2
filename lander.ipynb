{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "import PIL.Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 0              # seed for pseudo-random number generator\n",
    "MINIBATCH_SIZE = 64   # mini-batch size\n",
    "TAU = 1e-3            # soft update parameter\n",
    "E_DECAY = 0.995       # ε decay rate for ε-greedy policy\n",
    "E_MIN = 0.01          # minimum ε value for ε-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Action Space\n",
    "\n",
    "The agent has four discrete actions available:\n",
    "\n",
    "* Do nothing.\n",
    "* Fire right engine.\n",
    "* Fire main engine.\n",
    "* Fire left engine.\n",
    "\n",
    "Each action has a corresponding numerical value:\n",
    "\n",
    "```python\n",
    "Do nothing = 0\n",
    "Fire right engine = 1\n",
    "Fire main engine = 2\n",
    "Fire left engine = 3\n",
    "```\n",
    "\n",
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Observation Space\n",
    "\n",
    "The agent's observation space consists of a state vector with 8 variables:\n",
    "\n",
    "* Its $(x,y)$ coordinates. The landing pad is always at coordinates $(0,0)$.\n",
    "* Its linear velocities $(\\dot x,\\dot y)$.\n",
    "* Its angle $\\theta$.\n",
    "* Its angular velocity $\\dot \\theta$.\n",
    "* Two booleans, $l$ and $r$, that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Rewards\n",
    "\n",
    "The Lunar Lander environment has the following reward system:\n",
    "\n",
    "* Landing on the landing pad and coming to rest is about 100-140 points.\n",
    "* If the lander moves away from the landing pad, it loses reward. \n",
    "* If the lander crashes, it receives -100 points.\n",
    "* If the lander comes to rest, it receives +100 points.\n",
    "* Each leg with ground contact is +10 points.\n",
    "* Firing the main engine is -0.3 points each frame.\n",
    "* Firing the side engine is -0.03 points each frame.\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Episode Termination\n",
    "\n",
    "An episode ends (i.e the environment enters a terminal state) if:\n",
    "\n",
    "* The lunar lander crashes (i.e if the body of the lunar lander comes in contact with the surface of the moon).\n",
    "\n",
    "* The lander's $x$-coordinate is greater than 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make('LunarLander-v2', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's state size is (8,) and can take 4 actions!\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "state_size = env.observation_space.shape\n",
    "print(f\"Agent's state size is {state_size} and can take {num_actions} actions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment and get the initial state.\n",
    "initial_state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the environment is reset, the agent can start taking actions in the environment by using the `.step()` method. Note that the agent can only take one action per time step. \n",
    "\n",
    "In the cell below you can select different actions and see how the returned values change depending on the action taken. Remember that in this environment the agent has four discrete actions available and we specify them in code by using their corresponding numerical value:\n",
    "\n",
    "```python\n",
    "Do nothing = 0\n",
    "Fire right engine = 1\n",
    "Fire main engine = 2\n",
    "Fire left engine = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_names = [\"Did nothing\", \"Fired Right Engine\", \"Fired Main Engine\", \"Fired Left Engine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did nothing\n",
      "Next state: [ 0.00969496  1.4212197   0.49031258  0.21601267 -0.01110563 -0.10991845\n",
      "  0.          0.        ]\n",
      "Reward Recieved: 0.10807961634236563\n",
      "Episode Terminated: False\n",
      "Info: {}\n"
     ]
    }
   ],
   "source": [
    "action = 0\n",
    "next_state, reward, terminated, truncated, info = env.step(action)\n",
    "print(f\"{action_names[action]}\\nNext state: {next_state}\\nReward Recieved: {reward}\\nEpisode Terminated: {terminated}\\nInfo: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=64, activation=\"relu\"),\n",
    "    Dense(units=64, activation=\"relu\"),\n",
    "    Dense(units=num_actions)\n",
    "])\n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=64, activation=\"relu\"),\n",
    "    Dense(units=64, activation=\"relu\"),\n",
    "    Dense(units=num_actions)\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "    y_j =\n",
    "    \\begin{cases}\n",
    "      R_j & \\text{if episode terminates at step  } j+1\\\\\n",
    "      R_j + \\gamma \\max_{a'}\\hat{Q}(s_{j+1},a') & \\text{otherwise}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    states, actions , rewards, next_states, done_vals = experiences\n",
    "    y_targets = tf.reduce_max(target_q_network(next_states), axis=1)\n",
    "    y_targets = rewards + (gamma*y_targets*(1-done_vals))\n",
    "    \n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "    return MSE(q_values, y_targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_learn(experiences, gamma):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "    for target_weights, qnet_weights in zip(target_q_network.weights, q_network.weights):\n",
    "        target_weights.assign(TAU * qnet_weights + (1.0-TAU) * target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(q_values, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "        return np.argmax(q_values)\n",
    "    else:\n",
    "        return random.choice(np.arange(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=MINIBATCH_SIZE)\n",
    "    states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]),dtype=tf.float32)\n",
    "    done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "                                     dtype=tf.float32)\n",
    "    return (states, actions, rewards, next_states, done_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    total_points = 0\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    for t in range(max_num_timesteps):\n",
    "        q_vals = q_network(state.reshape(1,-1))\n",
    "        action = get_action(q_vals, epsilon)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        memory_buffer.append(experience(state, action, reward, next_state, terminated))\n",
    "        if (t+1) % NUM_STEPS_FOR_UPDATE == 0 and len(memory_buffer) > MINIBATCH_SIZE:\n",
    "            experiences = get_experiences(memory_buffer)\n",
    "            agent_learn(experiences, GAMMA)\n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "        if terminated:\n",
    "            break\n",
    "    total_point_history.append(total_points)\n",
    "    avg_latest_pts = np.mean(total_point_history[-num_p_av:])\n",
    "    epsilon = max(E_MIN, E_DECAY*epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {avg_latest_pts:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {avg_latest_pts:.2f}\")\n",
    "\n",
    "    if avg_latest_pts >= 200:\n",
    "        print(f\"Environment solved in {i+1} episodes!\")\n",
    "        break\n",
    "total_time = time.time() - start\n",
    "print(f\"\\nTotal Runtime: {total_time:.2f} s ({(total_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network.save('lunar_lander_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./lunar_lander.mp4\"\n",
    "import imageio\n",
    "import IPython\n",
    "import base64\n",
    "\n",
    "def create_video(filename, env, q_network, fps=30):\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        done = False\n",
    "        state = env.reset()[0]\n",
    "        frame = env.render()\n",
    "        video.append_data(frame)\n",
    "        while not done:    \n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            q_values = q_network(state)\n",
    "            action = np.argmax(q_values.numpy()[0])\n",
    "            state, _, done, truncated, _ = env.step(action)\n",
    "            frame = env.render()\n",
    "            video.append_data(frame)\n",
    "\n",
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "    video = open(filename,'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"840\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "create_video(filename, env, q_network)\n",
    "embed_mp4(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
